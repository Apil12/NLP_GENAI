{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "# Correctly accessing environment variables using os.getenv\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4.5-turbo-instruct\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert MCQ generator. When a user provides a topic, you will generate a multiple-choice question related to that topic. The question should include four options, labeled A, B, C, and D, with only one correct answer.\"),\n",
    "    (\"user\", \"Generate an MCQ for the topic: {topic}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-31 15:07:12.170 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:13.996 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Apil Thapa\\Desktop\\openai\\venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-08-31 15:07:14.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:14.011 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:14.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:14.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:14.026 Session state does not function when running a script without `streamlit run`\n",
      "2024-08-31 15:07:14.036 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-08-31 15:07:14.039 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate MCQ\n",
    "def generate_mcq(topic: str) -> dict:\n",
    "    prompt = prompt_template.format(topic=topic)\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    # Parse the response into a structured format\n",
    "    question = response.split(\"\\n\")[0].strip()\n",
    "    options = [opt.strip() for opt in response.split(\"\\n\")[1:5]]\n",
    "    answer = response.split(\"\\n\")[5].replace(\"Answer: \", \"\").strip()\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"MCQ Generator Chatbot\")\n",
    "\n",
    "# Step 1: User enters a topic\n",
    "topic = st.text_input(\"Enter a topic for MCQ generation\")\n",
    "\n",
    "if topic:\n",
    "    # Generate the first MCQ\n",
    "    mcq = generate_mcq(topic)\n",
    "    \n",
    "    # Initialize session state to keep track of MCQs and scores\n",
    "    if \"mcqs\" not in st.session_state:\n",
    "        st.session_state.mcqs = []\n",
    "        st.session_state.current_index = 0\n",
    "        st.session_state.correct_answers = 0\n",
    "        st.session_state.total_questions = 0\n",
    "\n",
    "    # Store the generated MCQ\n",
    "    st.session_state.mcqs.append(mcq)\n",
    "    st.session_state.total_questions += 1\n",
    "\n",
    "    # Display the current question\n",
    "    st.write(f\"Q{st.session_state.current_index + 1}: {st.session_state.mcqs[st.session_state.current_index]['question']}\")\n",
    "\n",
    "    # Display options as radio buttons\n",
    "    selected_option = st.radio(\n",
    "        \"Choose your answer:\",\n",
    "        options=st.session_state.mcqs[st.session_state.current_index]['options']\n",
    "    )\n",
    "\n",
    "    # Submit button\n",
    "    if st.button(\"Submit Answer\"):\n",
    "        correct_answer = st.session_state.mcqs[st.session_state.current_index]['answer']\n",
    "\n",
    "        # Check if the selected answer is correct\n",
    "        if selected_option.startswith(correct_answer[0]):\n",
    "            st.success(f\"Correct! The right answer is {correct_answer}.\")\n",
    "            st.session_state.correct_answers += 1\n",
    "        else:\n",
    "            st.error(f\"Wrong! The correct answer is {correct_answer}.\")\n",
    "        \n",
    "        # Move to the next question\n",
    "        st.session_state.current_index += 1\n",
    "\n",
    "        if st.session_state.current_index < st.session_state.total_questions:\n",
    "            # Show the next question\n",
    "            st.experimental_rerun()\n",
    "        else:\n",
    "            st.write(\"No more questions! Please enter a new topic to generate more MCQs.\")\n",
    "\n",
    "    # Show final score\n",
    "    if st.button(\"Show Final Score\"):\n",
    "        st.write(f\"Your final score is {st.session_state.correct_answers} out of {st.session_state.total_questions}.\")\n",
    "        st.session_state.current_index = 0\n",
    "        st.session_state.correct_answers = 0\n",
    "        st.session_state.total_questions = 0\n",
    "        st.session_state.mcqs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
