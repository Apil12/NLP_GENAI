# Fine-Tuning Large Language Models (LLMs) on Colab's Free GPU (Including 7B Parameters with Inference)

Welcome! Here, you'll learn how to fine-tune any large language model (LLM) for your specific needs using your own dataset. Whether it's for your business, school, or any other endeavor, you can create a customized Q&A chatbot tailored to your requirements. Let's explore how you can structure a chat template and fine-tune an LLM model using Hugging Face Transformers.

## 1. Fine-Tuning LLMs with Llama 2 Chat Model

| Topic                                   | Link              |
|-----------------------------------------|-------------------|
| Fine-Tune Llama 2 Chat Model (Code)     | [ðŸ”—](#)           |
| Example Dataset                         | [ðŸ”—](#)           |
| Llama Accepting Template Example        | [ðŸ”—](#)           |
| Code for Llama Accepting CSV Formation  | [ðŸ”—](#)           |
| Training Code                           | [ðŸ”—](#)           |
| Testing Code                            | [ðŸ”—](#)           |

## 1.1 Access Chat Templates for Fine-Tuning Various Models

| Notebook File                           | Link              |
|----------------------------------------|-------------------|
| Get Chat Template for LLMs Fine-Tuning (Code) | [ðŸ”—](#)           |

## a) Steps for Fine-Tuning Llama 2:

Steps | Description | Resources
------|-------------|----------
1) | Gather questions and answers in a text file | [ðŸ”—](#)
2) | Convert the text file into a format suitable for Llama 2 | [ðŸ”—](#)
3) | Share the prepared dataset on Hugging Face's repository | -
4) | Execute the provided Jupyter code to fine-tune Llama 2 | [ðŸ”—](#)
5) | Evaluate the performance of the fine-tuned model | [ðŸ”—](#)

## b) Steps for Fine-Tuning Gemma:

Steps | Description | Resources
------|-------------|----------
1) | Organize questions and answers in a text file | [ðŸ”—](#)
2) | Format the text file to match Gemma's requirements | [ðŸ”—](#)
3) | Share the formatted dataset on Hugging Face's repository | -
4) | Run the provided Jupyter code to fine-tune Gemma | [ðŸ”—](#)
5) | Assess the performance of the fine-tuned model | [ðŸ”—](#)

## c) Steps for Fine-Tuning Mistral (7 billion parameters model) on Colab's Free GPU with Inference:

Steps | Description | Resources
------|-------------|----------
1) | Compile questions and answers in a text file | [ðŸ”—](#)
2) | Convert the text file into a format compatible with Mistral | [ðŸ”—](#)
3) | Upload the formatted dataset to Hugging Face's repository | -
4) | Execute the provided Jupyter code to fine-tune Mistral and upload adapter files to Hugging Face's models | [ðŸ”—](#)
5) | Evaluate the fine-tuned model using PeftModel | [ðŸ”—](#)
