{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYvy0i1xQ9AK"
      },
      "outputs": [],
      "source": [
        "!pip install ctransformers ctransformers[cuda]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ctransformers import AutoModelForCausalLM\n",
        "\n",
        "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGUF\",\n",
        "                                           model_file=\"llama-2-7b.Q2_K.gguf\",\n",
        "                                           max_new_tokens=50 ,\n",
        "                                           gpu_layers=100) # we dont use -1 here for gpu for all layers"
      ],
      "metadata": {
        "id": "t4XVlV3IREgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm(\"write a paragraph about gulmi\")"
      ],
      "metadata": {
        "id": "722FTVTIREi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install ctransformers ctransformers[cuda]"
      ],
      "metadata": {
        "id": "FHBzn_UhREmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import CTransformers"
      ],
      "metadata": {
        "id": "TpWC8sIXRUyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {'max_new_tokens': 256, 'repetition_penalty': 1.1, 'gpu_layers':100}\n",
        "# for more parameters in config go= https://github.com/marella/ctransformers\n",
        "llm = CTransformers(model='TheBloke/Llama-2-7B-Chat-GGML', model_file='llama-2-7b-chat.ggmlv3.q5_1.bin',config=config)"
      ],
      "metadata": {
        "id": "1i0Fe2ToRU05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"i want to live in\")"
      ],
      "metadata": {
        "id": "Dd5L8JR7RU4o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}